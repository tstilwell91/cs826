{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0d1edb-b8db-4767-9faa-ddb2e206b364",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading WSI paths...\n",
      "Found 8 WSI paths across subdirectories.\n",
      "Initialized WSIDataset with 8 WSIs.\n",
      "Number of tiles per WSI: 8\n",
      "Total number of WSIs: 8\n",
      "Batch size: 4 (Defined by the number of WSIs processed in parallel during feature extraction)\n",
      "Number of batches per epoch: 2 (Calculated as total number of WSIs 8 divided by batch size 4, rounded up)\n",
      "Loading pretrained ResNet18 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for feature extraction.\n",
      "Starting feature extraction with Attention MIL...\n",
      "Starting epoch 1/1...\n",
      "Processing batch 1 of epoch 1... (using device: cuda)\n",
      "Batch moved to cuda. Performing forward pass...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'entity_submitter_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entity_submitter_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 164\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Extract features with Attention MIL\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting feature extraction with Attention MIL...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_with_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet18\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Save features as .csv\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving features as .csv...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 150\u001b[0m, in \u001b[0;36mextract_features_with_attention\u001b[0;34m(dataloader, model, attention_model, device, epochs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m matching_rows\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m    149\u001b[0m     case_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 150\u001b[0m     entity_submitter_id \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentity_submitter_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    151\u001b[0m     aggregated_feature \u001b[38;5;241m=\u001b[39m aggregated_features[i]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    152\u001b[0m     feature_row \u001b[38;5;241m=\u001b[39m [case_id, entity_submitter_id] \u001b[38;5;241m+\u001b[39m aggregated_feature\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entity_submitter_id'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from openslide import OpenSlide\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Avoid DecompressionBombError\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Configuration parameters\n",
    "DATA_DIR = \"/home/tstil004/phd/multi-omics/slides/\"  # Directory containing the TCGA dataset\n",
    "CASE_MAPPING_FILE = '/home/tstil004/phd/multi-omics/file_case_mapping.csv'  # Path to the case mapping file\n",
    "BATCH_SIZE = 4  # Number of WSIs processed in parallel during feature extraction. Increase to utilize GPU memory better if available.\n",
    "NUM_WORKERS = 8  # Number of worker processes used by the DataLoader to load data in parallel. Increase to speed up data loading.\n",
    "EPOCHS = 1  # Number of times the entire dataset is passed through the model. More epochs ensure better coverage of the dataset.\n",
    "NUM_TILES = 8  # Number of random tiles sampled from each WSI per epoch. More tiles provide better representation but require more memory.\n",
    "FEATURE_DIM = 512  # Dimensionality of the feature vector extracted from each tile by the ResNet18 model.\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the case mapping file\n",
    "case_mapping_df = pd.read_csv(CASE_MAPPING_FILE)\n",
    "\n",
    "# Define a custom Dataset to handle WSIs\n",
    "class WSIDataset(Dataset):\n",
    "    def __init__(self, wsi_paths, transform=None, num_tiles=NUM_TILES):\n",
    "        self.wsi_paths = wsi_paths\n",
    "        self.transform = transform\n",
    "        self.num_tiles = num_tiles\n",
    "        print(f\"Initialized WSIDataset with {len(self.wsi_paths)} WSIs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wsi_paths)  # Dataset length is based on the number of WSIs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wsi_path = self.wsi_paths[idx]\n",
    "        slide = OpenSlide(wsi_path)\n",
    "        width, height = slide.dimensions\n",
    "        tiles = []\n",
    "        for _ in range(self.num_tiles):\n",
    "            # Calculate random coordinates to extract a tile\n",
    "            tile_size = min(width, height) // 10  # Define tile size\n",
    "            x = random.randint(0, max(0, width - tile_size))\n",
    "            y = random.randint(0, max(0, height - tile_size))\n",
    "\n",
    "            # Extract a tile from the WSI\n",
    "            tile = slide.read_region((x, y), 0, (tile_size, tile_size)).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                tile = self.transform(tile)\n",
    "            tiles.append(tile)\n",
    "\n",
    "        tiles = torch.stack(tiles)  # Shape: (NUM_TILES, 3, 224, 224)\n",
    "        return wsi_path, tiles\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load WSI paths\n",
    "print(\"Loading WSI paths...\")\n",
    "wsi_paths = []\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith(\".svs\") or file.endswith(\".tif\"):\n",
    "            wsi_paths.append(os.path.join(root, file))\n",
    "if len(wsi_paths) == 0:\n",
    "    raise ValueError(f\"No WSI files found in {DATA_DIR}. Please check the directory and ensure it contains subdirectories with files with supported extensions.\")\n",
    "print(f\"Found {len(wsi_paths)} WSI paths across subdirectories.\")\n",
    "dataset = WSIDataset(wsi_paths, transform=transform)\n",
    "\n",
    "total_dataset_size = len(dataset)  # Total number of WSIs\n",
    "num_batches_per_epoch = (total_dataset_size + BATCH_SIZE - 1) // BATCH_SIZE  # Calculate the number of batches per epoch\n",
    "\n",
    "print(f\"Number of tiles per WSI: {NUM_TILES}\")\n",
    "print(f\"Total number of WSIs: {total_dataset_size}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (Defined by the number of WSIs processed in parallel during feature extraction)\")\n",
    "print(f\"Number of batches per epoch: {num_batches_per_epoch} (Calculated as total number of WSIs {total_dataset_size} divided by batch size {BATCH_SIZE}, rounded up)\")\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Load pretrained ResNet18 model and modify it for feature extraction\n",
    "print(\"Loading pretrained ResNet18 model...\")\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18 = nn.Sequential(*list(resnet18.children())[:-1])  # Remove the final classification layer\n",
    "resnet18 = resnet18.to(device)\n",
    "resnet18.eval()\n",
    "print(\"Model loaded and ready for feature extraction.\")\n",
    "\n",
    "# Attention MIL module\n",
    "class AttentionMIL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionMIL, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_tiles, feature_dim)\n",
    "        attn_weights = self.attention(x)  # Shape: (batch_size, num_tiles, 1)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # Apply softmax across tiles\n",
    "        weighted_features = torch.sum(attn_weights * x, dim=1)  # Weighted sum of tile features\n",
    "        return weighted_features\n",
    "\n",
    "# Feature extraction loop with Attention MIL\n",
    "def extract_features_with_attention(dataloader, model, attention_model, device, epochs):\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Starting epoch {epoch + 1}/{epochs}...\")\n",
    "            for batch_idx, (wsi_paths, batch) in enumerate(dataloader):\n",
    "                print(f\"Processing batch {batch_idx + 1} of epoch {epoch + 1}... (using device: {device})\")\n",
    "                batch = batch.to(device, non_blocking=True)  # Shape: (BATCH_SIZE, NUM_TILES, 3, 224, 224)\n",
    "                print(f\"Batch moved to {device}. Performing forward pass...\")\n",
    "                batch_size, num_tiles, _, _, _ = batch.shape\n",
    "                batch = batch.view(batch_size * num_tiles, 3, 224, 224)  # Reshape to process all tiles\n",
    "                features = model(batch)  # Extract features from ResNet18\n",
    "                features = features.view(batch_size, num_tiles, -1)  # Shape: (BATCH_SIZE, NUM_TILES, FEATURE_DIM)\n",
    "                aggregated_features = attention_model(features)  # Aggregate tile features using Attention MIL\n",
    "\n",
    "                # Extract case_id and entity_submitter_id from wsi_paths using the mapping file\n",
    "                for i, wsi_path in enumerate(wsi_paths):\n",
    "                    wsi_file_name = os.path.basename(wsi_path)\n",
    "                    matching_rows = case_mapping_df.loc[case_mapping_df['file_name'] == wsi_file_name]\n",
    "                    if len(matching_rows) == 0:\n",
    "                        print(f\"Warning: No matching case_id found for WSI {wsi_file_name}. Skipping.\")\n",
    "                        continue\n",
    "                    for _, row in matching_rows.iterrows():\n",
    "                        case_id = row['case_id']\n",
    "                        entity_submitter_id = row['entity_submitter_id']\n",
    "                        aggregated_feature = aggregated_features[i].cpu().numpy()\n",
    "                        feature_row = [case_id, entity_submitter_id] + aggregated_feature.tolist()\n",
    "                        all_features.append(feature_row)\n",
    "                print(f\"Batch {batch_idx + 1} of epoch {epoch + 1} processed.\")\n",
    "\n",
    "    return all_features\n",
    "\n",
    "# Initialize Attention MIL model\n",
    "attention_mil = AttentionMIL(input_dim=FEATURE_DIM, hidden_dim=256).to(device)\n",
    "attention_mil.eval()\n",
    "\n",
    "# Extract features with Attention MIL\n",
    "print(\"Starting feature extraction with Attention MIL...\")\n",
    "features = extract_features_with_attention(dataloader, resnet18, attention_mil, device, EPOCHS)\n",
    "\n",
    "# Save features as .csv\n",
    "print(\"Saving features as .csv...\")\n",
    "columns = ['case_id', 'entity_submitter_id'] + [f'feature_{i}' for i in range(FEATURE_DIM)]\n",
    "features_df = pd.DataFrame(features, columns=columns)\n",
    "features_df.to_csv('extracted_wsi_features2.csv', index=False)\n",
    "\n",
    "print(\"Feature extraction complete. Saved to 'extracted_wsi_features2.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
